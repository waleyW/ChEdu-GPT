import pandas as pd
import sqlite3
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import gradio as gr
import os
# 读取CSV文件，指定编码格式
csv_file_path = r'/home/ubuntu/mnt2/wxy/Ako_GPT/data/exam_entry_answer.csv'  # 替换为您的CSV文件路径
data = pd.read_csv(csv_file_path, encoding='ISO-8859-1')  # 可以使用 'latin1' 或 'gbk' 作为替代

# 初始化数据库并插入数据的函数
def initialize_database():
    conn = sqlite3.connect('questions.db')
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS questions (
        entry TEXT PRIMARY KEY,
        answer TEXT
    )
    ''')
    # 将CSV数据插入到数据库中
    for index, row in data.iterrows():
        cursor.execute('INSERT OR IGNORE INTO questions (entry, answer) VALUES (?, ?)', (row['entry'], row['answer']))
    conn.commit()
    conn.close()

# 初始化数据库
initialize_database()

# 根据题号检索答案的函数
def get_answer(entry_id):
    conn = sqlite3.connect('questions.db')
    cursor = conn.cursor()
    cursor.execute("SELECT answer FROM questions WHERE entry=?", (entry_id,))
    result = cursor.fetchone()
    conn.close()
    print(f"Retrieved answer: {result}")  # 调试输出
    if result:
        return result[0]
    else:
        return None


# 加载LLM模型的函数
def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch_dtype = torch.float16
    else:
        device = torch.device("cpu")
        torch_dtype = torch.float32

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch_dtype
    ).to(device)
    
    return model, tokenizer, device

# 加载模型
model_path = r'/home/ubuntu/mnt2/wxy/model/Mistral-7B-Instruct-v0.3'  
model, tokenizer, device = load_model(model_path)

# 处理输入并调用LLM生成回答的函数
def ask(text):
    if not isinstance(text, str):  # Ensure the input is a string
        return "Input text must be a valid string."

    instruction_prefix = (
        "<s>[INST] <<SYS>>\n"
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\n"
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n"
        "<</SYS>> \n\n"
    )
    
    pdf_text = ''  # Placeholder as pdf extraction is not currently implemented
    
    # 从用户的输入中提取题号并检索答案
    match = re.search(r'question ID is ([\w-]+)', text)
    if match:
        entry_id = match.group(1)
        answer = get_answer(entry_id)
        if answer:
            combined_prompt = f"{instruction_prefix}{pdf_text}\n\nquestion ID is {entry_id}'s answer: {answer}\n\nuser question: {text}[/INST]"
        else:
            combined_prompt = f"{instruction_prefix}{pdf_text}{text} [/INST]"
    else:
        combined_prompt = f"{instruction_prefix}{pdf_text}{text} [/INST]"

    inputs = tokenizer(combined_prompt, return_tensors='pt').to(device)
    input_length = inputs.input_ids.shape[1]
    outputs = model.generate(**inputs, max_new_tokens=2000, temperature=0.7, return_dict_in_generate=True)
    tokens = outputs.sequences[0, input_length:]
    return tokenizer.decode(tokens)
# Disclaimer text
disclaimer_text = """
This LLM (Large Language Model) inferencing tool is provided for educational and research purposes only. Please read and understand the following disclaimers before using this service:

1. The responses generated by this AI model are not guaranteed to be accurate, complete, or suitable for any specific purpose.
2. Users are solely responsible for verifying and validating any information or suggestions provided by the AI model.
3. This service should not be used as a substitute for professional advice in any field, including but not limited to medical, legal, financial, or educational matters.
4. The developers and operators of this service are not responsible for any consequences resulting from the use of the information provided by the AI model.
5. User data may be collected and processed for the purpose of improving the service. By using this tool, you consent to such data collection and processing.
6. The service may be interrupted, terminated, or modified at any time without prior notice.
7. Users are prohibited from using this service for any illegal, unethical, or harmful purposes.
8. Personal data has been processed and sensitive information has been removed.

By using this LLM inferencing tool, you acknowledge that you have read, understood, and agreed to this disclaimer.
"""

# Feature introduction text
feature_intro = """
## Ako-GPT Features:

1. **Answer Piazza Questions**: Supports answering existing questions from Piazza (under optimization).

2. **Exam Question Assistance**: Provides help with exam-related questions. 
   **If you want to ask for answers to exam questions, please follow the format: question ID is xxxx (for example: CHEM251_2023_Exam-1-b-2)**

3. **Basic Chemistry Knowledge**: Offers support for fundamental chemistry concepts and questions.
"""

# Get the directory of the current script
script_dir = os.path.dirname(os.path.abspath(__file__))

# Construct the path to the image file
image_path = os.path.join(script_dir, "ako-gpt-logo.png")

# Gradio Web Interface
with gr.Blocks() as server:
    with gr.Row():
        gr.Image(image_path, show_label=False, width=50)
        gr.Markdown("# Ako-GPT")
    
    gr.Markdown(feature_intro)
    
    gr.Markdown("<br>")  # Add some space

    with gr.Tab("LLM Inferencing"):
        model_input = gr.Textbox(label="Your Question:", placeholder="Enter your question here", interactive=True)
        ask_button = gr.Button("Ask")
        model_output = gr.Textbox(label="The Answer:", interactive=False, placeholder="The answer will appear here...")
        
        ask_button.click(fn=ask, inputs=model_input, outputs=model_output)
    
    # Add some vertical space
    for _ in range(3):
        gr.Markdown("<br>")
    
    # Add disclaimer at the bottom
    with gr.Row():
        gr.Markdown("---")  # Horizontal line for separation
    with gr.Accordion("Disclaimer", open=False):
        gr.Markdown(disclaimer_text)

server.launch(share=True)